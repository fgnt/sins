{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from pprint import pprint\n",
    "\n",
    "from sins.database.database import SINS, AudioReader\n",
    "from sins.database.utils import prepare_sessions\n",
    "from sins.systems.sad.model import BinomialClassifier\n",
    "from sins.systems.modules import CNN2d, CNN1d, AutoPool\n",
    "from sins.features.stft import STFT\n",
    "from sins.features.mel_transform import MelTransform\n",
    "from sins.features.normalize import Normalizer\n",
    "from sins.systems.utils import Collate\n",
    "from sins import paths\n",
    "from sins.systems.sad.utils import load_sections, get_sections_in_range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = paths.exp_dir / 'sad' / '2019-10-07-06-57-23'\n",
    "with (exp_dir / '1' / 'config.json').open() as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinomialClassifier(\n",
    "    cnn_2d=CNN2d(**config['model']['cnn_2d']),\n",
    "    cnn_1d=CNN1d(**config['model']['cnn_1d']),\n",
    "    pooling=AutoPool(**config['model']['pool'])\n",
    ")\n",
    "ckpt = torch.load(exp_dir / 'ckpt-best.pth')\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval()\n",
    "model.pooling.alpha = 2.0\n",
    "\n",
    "pool_sizes = [\n",
    "    pool_size[1] if isinstance(pool_size, (list, tuple)) else pool_size\n",
    "    for pool_size in (model._cnn_2d.pool_sizes + model._cnn_1d.pool_sizes)\n",
    "]\n",
    "total_pool_size = np.prod(pool_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SINS()\n",
    "presence = prepare_sessions(\n",
    "    db.sessions, room='living', include_absence=True,\n",
    "    discard_other_rooms=True, discard_ambiguities=False,\n",
    "    label_map_fn=lambda label: (False if label == \"absence\" else True)\n",
    ")\n",
    "eval_sets = db.get_segments(\n",
    "    db.room_to_nodes['living'], max_segment_length=60., time_ranges=db.eval_ranges, sessions=presence,\n",
    "    session_key='presence'\n",
    ")\n",
    "\n",
    "audio_reader = AudioReader(**config['audio_reader'])\n",
    "stft = STFT(**config['stft'])\n",
    "mel_transform = MelTransform(**config['mel_transform'])\n",
    "normalizers = [\n",
    "    Normalizer(\"mel_transform\", name=node, **config['normalizer']) for node in db.room_to_nodes['living']\n",
    "]\n",
    "[normalizer.initialize_moments(verbose=True) for normalizer in normalizers]\n",
    "\n",
    "def pretend_batch(example):\n",
    "    example['features'] = torch.Tensor(example['mel_transform'].transpose((0,2,1))[:, None])\n",
    "    example['seq_len'] = 4*[example['mel_transform'].shape[-1]]\n",
    "    return example\n",
    "\n",
    "eval_sets = [\n",
    "    ds.map(audio_reader).map(stft).map(mel_transform).map(normalizer).map(pretend_batch)\n",
    "    for ds, normalizer in zip(eval_sets, normalizers)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = eval_sets[0][200]\n",
    "print(example['presence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sad, seq_len = model(example)\n",
    "sad = sad.cpu().data.numpy().mean((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_on, n_off = 0, 300\n",
    "x = example['mel_transform'][0, 10*n_on:10*n_off].T\n",
    "y = sad[n_on:n_off]\n",
    "fig, axes = plt.subplots(2,1)\n",
    "axes[0].imshow(x, interpolation='nearest', aspect='auto', origin=\"lower\")\n",
    "axes[1].plot(y, linewidth=2)\n",
    "axes[1].set_xlim([-0.5, y.shape[0]-0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sins.systems.sad.evaluate import prepare_dataset, fscore, prepare_targets, simple_sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_events_dir = '/path/to/dcase2016_task2_train_sounds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.array([[[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]]])\n",
    "targets = prepare_targets(\n",
    "    scores, onset_frames=np.array([6]), offset_frames=np.array([12.])\n",
    ")\n",
    "print(targets)\n",
    "print(fscore(targets, scores>0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot example mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_sets, onset_frames, offset_frames = prepare_dataset(\n",
    "    exp_dir, sound_events_dir, mixtures_per_sound=1, segment_length=20., snr=0.,\n",
    "    nodes=[\"Node1\"], num_workers=8, prefetch_buffer=16, seed=0, dataset_name='eval',\n",
    "    prefetch=False\n",
    ")\n",
    "example_idx = 100\n",
    "batch = eval_sets[0][example_idx]\n",
    "y, seq_len = model(Collate()(batch))\n",
    "y = y.cpu().data.numpy().mean((0,1))\n",
    "x = batch[1]['features'][0]\n",
    "targets = prepare_targets(\n",
    "    y[None,None], \n",
    "    np.array([onset_frames[example_idx] / total_pool_size]),\n",
    "    np.array([offset_frames[example_idx] / total_pool_size])\n",
    ")[0, 0]\n",
    "print(targets.shape)\n",
    "\n",
    "n_on = 0 \n",
    "n_off = 1000\n",
    "x = x[:, n_on:n_off]\n",
    "y = y[n_on//total_pool_size:n_off//total_pool_size]\n",
    "targets = targets[n_on//total_pool_size:n_off//total_pool_size]\n",
    "\n",
    "fig, axes = plt.subplots(3,1)\n",
    "axes[0].imshow(\n",
    "    x,#[...,int(onset_frames[n]-10):int(offset_frames[n]+10)],\n",
    "    interpolation='nearest', aspect='auto', origin=\"lower\"\n",
    ")\n",
    "axes[1].plot(targets, linewidth=2)\n",
    "axes[1].set_ylim([0., 1.])\n",
    "axes[1].set_xlim([-0.5, targets.shape[-1]-0.5])\n",
    "axes[2].plot(y, linewidth=2)\n",
    "axes[2].set_ylim([0., 1.])\n",
    "axes[2].set_xlim([-0.5, y.shape[-1]-0.5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listen to active sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = config[\"nodes\"]\n",
    "sections = load_sections(exp_dir / 'ensemble_sections.json')\n",
    "train_sections = get_sections_in_range(sections, db.train_ranges)\n",
    "datasets = db.get_segments(\n",
    "    nodes, min_segment_length=1., max_segment_length=60., \n",
    "    time_ranges=train_sections\n",
    ")\n",
    "datasets = [ds.map(audio_reader) for ds in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_idx = 100\n",
    "for ds in datasets:\n",
    "    ipd.display(ipd.Audio(ds[sec_idx]['audio_data'][0], rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlate Node sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlap(sections_a, sections_b, shift=0.):\n",
    "    sections_b = [(start - shift, stop - shift) for (start, stop) in sections_b]\n",
    "    overlap = 0.\n",
    "    idx_b = 0\n",
    "    for section_a in sections_a:\n",
    "        while idx_b < len(sections_b) and sections_b[idx_b][-1] < section_a[-2]:\n",
    "            idx_b += 1\n",
    "        while idx_b < len(sections_b) and sections_b[idx_b][-1] < section_a[-1]:\n",
    "            overlap += sections_b[idx_b][-1] - max(section_a[-2], sections_b[idx_b][-2])\n",
    "            idx_b += 1\n",
    "        if idx_b < len(sections_b):\n",
    "            overlap += max(\n",
    "                section_a[-1] - max(section_a[-2], sections_b[idx_b][-2]), 0.)\n",
    "    return overlap\n",
    "\n",
    "def correlate_segments(sections_a, sections_b, shifts=np.arange(-1., 1.1, .2)):\n",
    "    return np.array([\n",
    "        compute_overlap(sections_a, sections_b, shift) for shift in shifts\n",
    "    ])\n",
    "\n",
    "def compute_shift_mat(sections, shifts=np.arange(-1., 1.1, .2)):\n",
    "    shift_mat = np.array([\n",
    "        [\n",
    "            shifts[np.argmax(correlate_segments(\n",
    "                sections[node_1], sections[node_2], shifts\n",
    "            ))] for node_2 in range(len(sections))\n",
    "        ]\n",
    "        for node_1 in range(len(sections))\n",
    "    ])\n",
    "    return shift_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_sections = []\n",
    "for node in db.room_to_nodes['living']:\n",
    "    with (exp_dir / f'{node}_sections.json').open() as f:\n",
    "        node_sections.append(json.load(f))\n",
    "shifts = np.arange(-1., 1.1, .1)\n",
    "autocorrelation = correlate_segments(node_sections[5], node_sections[5], shifts=shifts)\n",
    "crosscorrelation = correlate_segments(node_sections[5], node_sections[6], shifts=shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(6,2))\n",
    "ax.plot(shifts, autocorrelation/1000)\n",
    "ax.plot(shifts, crosscorrelation/1000)\n",
    "ax.set_xlim(-1,1)\n",
    "ax.set_xlabel('Lag / s', size=14)\n",
    "ax.set_ylabel('Correlation / ($10^3$ s)', size=14)\n",
    "ax.legend(['Auto', 'Cross'], fontsize=13)\n",
    "ax.tick_params(axis='both', which='major', labelsize=13)\n",
    "ax.grid()\n",
    "plt.locator_params(axis='x', nbins=5)\n",
    "plt.savefig('correlation.pdf', bbox_inches = 'tight', pad_inches = 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_mat = compute_shift_mat(node_sections)\n",
    "shifts = shift_mat[0]\n",
    "shift_mat = shift_mat - shifts + shifts[:, None]\n",
    "assert (np.abs(shift_mat) < 0.1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sins",
   "language": "python",
   "name": "sins"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
